import streamlit as st
import requests
from bs4 import BeautifulSoup
from PIL import Image
import io
import base64
import json
import time
from typing import List, Dict, Any, Tuple, Optional

# --- C·∫•u h√¨nh API Gemini ---
# KH√îNG C·∫¶N CH·ªà ƒê·ªäNH API KEY. Streamlit Cloud s·∫Ω t·ª± ƒë·ªông cung c·∫•p trong m√¥i tr∆∞·ªùng ch·∫°y.
GEMINI_MODEL = "gemini-2.5-flash-preview-09-2025"
GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent"
API_KEY = "" # S·∫Ω ƒë∆∞·ª£c Canvas cung c·∫•p t·ª± ƒë·ªông

# --- Thi·∫øt l·∫≠p giao di·ªán Streamlit ---
st.set_page_config(
    page_title="Tr√¨nh T·∫£i & Ph√¢n t√≠ch H√¨nh ·∫£nh Web",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
.stButton>button {
    background-color: #4CAF50;
    color: white;
    font-weight: bold;
    border-radius: 8px;
    padding: 10px 24px;
    transition: all 0.3s ease;
}
.stButton>button:hover {
    background-color: #45a049;
    box-shadow: 0 4px 8px 0 rgba(0,0,0,0.2);
}
</style>
""", unsafe_allow_html=True)

st.title("üì∏ C√¥ng c·ª• T·∫£i v√† Ph√¢n t√≠ch H√¨nh ·∫£nh Web")
st.markdown("D√°n URL c·ªßa trang web b·∫°n mu·ªën tr√≠ch xu·∫•t h√¨nh ·∫£nh, sau ƒë√≥ s·ª≠ d·ª•ng c√°c b·ªô l·ªçc v√† c√¥ng c·ª• AI.")

# Kh·ªüi t·∫°o state session
if 'extracted_images' not in st.session_state:
    st.session_state.extracted_images = []
if 'analyzed_images' not in st.session_state:
    st.session_state.analyzed_images = []


# --- C√°c H√†m Ti·ªán √≠ch ---

def base64_to_inline_data(image_base64: str, mime_type: str = "image/jpeg") -> Dict[str, Dict[str, str]]:
    """T·∫°o c·∫•u tr√∫c inlineData cho API Gemini."""
    return {
        "inlineData": {
            "mimeType": mime_type,
            "data": image_base64
        }
    }

def get_image_data_and_base64(img_url: str) -> Optional[Tuple[bytes, int, int, str]]:
    """
    T·∫£i ·∫£nh, l·∫•y k√≠ch th∆∞·ªõc v√† chuy·ªÉn ƒë·ªïi th√†nh base64.
    Tr·∫£ v·ªÅ (bytes, width, height, base64_string) ho·∫∑c None n·∫øu th·∫•t b·∫°i.
    """
    try:
        response = requests.get(img_url, timeout=10)
        response.raise_for_status()
        img_bytes = response.content
        img_mime = response.headers.get('Content-Type', 'image/jpeg')

        # D√πng PIL ƒë·ªÉ l·∫•y k√≠ch th∆∞·ªõc
        img = Image.open(io.BytesIO(img_bytes))
        width, height = img.size

        # Chuy·ªÉn ƒë·ªïi th√†nh base64 cho API AI
        buffered = io.BytesIO()
        # L∆∞u l·∫°i d∆∞·ªõi d·∫°ng JPEG ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªãnh d·∫°ng t∆∞∆°ng th√≠ch v·ªõi API, n·∫øu kh√¥ng ph·∫£i GIF/PNG
        if 'image/png' in img_mime or 'image/gif' in img_mime:
             img.save(buffered, format=img.format or "PNG")
             img_mime = "image/png"
        else:
             img.save(buffered, format="JPEG")
             img_mime = "image/jpeg"
        
        base64_encoded = base64.b64encode(buffered.getvalue()).decode('utf-8')

        return img_bytes, width, height, base64_encoded, img_mime

    except Exception as e:
        # print(f"L·ªói khi x·ª≠ l√Ω ·∫£nh {img_url}: {e}")
        return None

# --- Ch·ª©c nƒÉng Web Scraping ---

def extract_images(url: str):
    """L·∫•y t·∫•t c·∫£ c√°c URL h√¨nh ·∫£nh t·ª´ m·ªôt trang web."""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        st.info(f"ƒêang c·ªë g·∫Øng t·∫£i n·ªôi dung t·ª´: {url}")
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        img_tags = soup.find_all('img')
        
        # L·ªçc v√† chu·∫©n h√≥a URL
        image_urls = []
        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if src and src.startswith(('http', 'https')):
                image_urls.append(src)
            elif src and not src.startswith(('mailto', 'tel', '#')):
                # X·ª≠ l√Ω URL t∆∞∆°ng ƒë·ªëi
                from urllib.parse import urljoin
                full_url = urljoin(url, src)
                image_urls.append(full_url)
        
        # Lo·∫°i b·ªè c√°c URL tr√πng l·∫∑p
        unique_urls = list(set(image_urls))
        st.success(f"ƒê√£ tr√≠ch xu·∫•t ƒë∆∞·ª£c {len(unique_urls)} URL h√¨nh ·∫£nh duy nh·∫•t.")
        
        return unique_urls

    except requests.exceptions.RequestException as e:
        st.error(f"L·ªói khi truy c·∫≠p URL: {e}")
        return []
    except Exception as e:
        st.error(f"ƒê√£ x·∫£y ra l·ªói: {e}")
        return []

# --- Ch·ª©c nƒÉng Ph√¢n t√≠ch H√¨nh ·∫£nh (AI) ---

def analyze_image_with_ai(base64_data: str, mime_type: str, retry_count: int = 5) -> str:
    """G·ªçi API Gemini ƒë·ªÉ ph√¢n t√≠ch h√¨nh ·∫£nh v√† tr·∫£ v·ªÅ m√¥ t·∫£."""
    
    prompt = "M√¥ t·∫£ chi ti·∫øt v√† ch√≠nh x√°c h√¨nh ·∫£nh n√†y b·∫±ng ti·∫øng Vi·ªát. T·∫≠p trung v√†o c√°c ƒë·ªëi t∆∞·ª£ng, h√†nh ƒë·ªông, v√† b·ªëi c·∫£nh."
    
    # Chu·∫©n b·ªã payload
    payload = {
        "contents": [
            {
                "role": "user",
                "parts": [
                    {"text": prompt},
                    base64_to_inline_data(base64_data, mime_type)
                ]
            }
        ],
    }

    headers = {'Content-Type': 'application/json'}
    
    for i in range(retry_count):
        try:
            # st.info(f"Th·ª≠ g·ªçi API l·∫ßn {i + 1}...")
            response = requests.post(GEMINI_API_URL, headers=headers, json=payload, timeout=30)
            response.raise_for_status() # N√©m ngo·∫°i l·ªá cho c√°c m√£ l·ªói HTTP 4xx/5xx
            
            result = response.json()
            candidate = result.get('candidates', [{}])[0]
            
            if candidate and candidate.get('content') and candidate['content'].get('parts'):
                text = candidate['content']['parts'][0].get('text', 'Kh√¥ng th·ªÉ t·∫°o m√¥ t·∫£.')
                return text
            else:
                # N·∫øu kh√¥ng c√≥ text, th·ª≠ l·∫°i ho·∫∑c tr·∫£ v·ªÅ l·ªói
                return "API tr·∫£ v·ªÅ c·∫•u tr√∫c r·ªóng ho·∫∑c kh√¥ng h·ª£p l·ªá."

        except requests.exceptions.RequestException as e:
            if response.status_code in [429, 500, 503] and i < retry_count - 1:
                wait_time = 2 ** i
                # st.warning(f"L·ªói t·∫°m th·ªùi ({response.status_code}). ƒê·ª£i {wait_time}s tr∆∞·ªõc khi th·ª≠ l·∫°i.")
                time.sleep(wait_time)
            else:
                # st.error(f"L·ªói g·ªçi API sau {i + 1} l·∫ßn th·ª≠: {e}")
                return f"L·ªói g·ªçi API: {e}"
        except Exception as e:
            # st.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
            return f"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}"
            
    return "Th·ª≠ l·∫°i th·∫•t b·∫°i. Vui l√≤ng ki·ªÉm tra API key ho·∫∑c ƒë·ª£i m·ªôt l√°t."


# --- Giao di·ªán v√† Logic ch√≠nh ---

with st.sidebar:
    st.header("‚öôÔ∏è Thi·∫øt l·∫≠p & B·ªô l·ªçc")
    input_url = st.text_input("URL Trang Web", "https://unsplash.com")

    st.subheader("L·ªçc K√≠ch th∆∞·ªõc H√¨nh ·∫£nh (Pixels)")
    col1, col2 = st.columns(2)
    with col1:
        min_width = st.number_input("Chi·ªÅu r·ªông t·ªëi thi·ªÉu (Min Width)", min_value=0, value=300)
    with col2:
        max_width = st.number_input("Chi·ªÅu r·ªông t·ªëi ƒëa (Max Width)", min_value=0, value=9999)
    
    col3, col4 = st.columns(2)
    with col3:
        min_height = st.number_input("Chi·ªÅu cao t·ªëi thi·ªÉu (Min Height)", min_value=0, value=300)
    with col4:
        max_height = st.number_input("Chi·ªÅu cao t·ªëi ƒëa (Max Height)", min_value=0, value=9999)

    if st.button("Tr√≠ch xu·∫•t H√¨nh ·∫£nh", use_container_width=True):
        if not input_url:
            st.error("Vui l√≤ng nh·∫≠p m·ªôt URL h·ª£p l·ªá.")
        else:
            with st.spinner("ƒêang tr√≠ch xu·∫•t v√† ki·ªÉm tra k√≠ch th∆∞·ªõc h√¨nh ·∫£nh..."):
                st.session_state.extracted_images = []
                st.session_state.analyzed_images = []
                
                urls = extract_images(input_url)
                
                # B·∫Øt ƒë·∫ßu t·∫£i v√† ki·ªÉm tra k√≠ch th∆∞·ªõc
                progress_bar = st.progress(0)
                image_data_list = []
                total_urls = len(urls)

                for i, img_url in enumerate(urls):
                    data = get_image_data_and_base64(img_url)
                    
                    if data:
                        img_bytes, width, height, base64_encoded, mime_type = data
                        
                        # Ki·ªÉm tra b·ªô l·ªçc
                        if (min_width <= width <= max_width) and (min_height <= height <= max_height):
                            image_data_list.append({
                                "url": img_url,
                                "bytes": img_bytes,
                                "width": width,
                                "height": height,
                                "base64": base64_encoded,
                                "mime_type": mime_type,
                                "analysis": "Ch∆∞a ph√¢n t√≠ch"
                            })
                    
                    progress_bar.progress((i + 1) / total_urls)
                
                st.session_state.extracted_images = image_data_list
                st.success(f"Ho√†n t·∫•t tr√≠ch xu·∫•t. ƒê√£ t√¨m th·∫•y v√† l·ªçc ƒë∆∞·ª£c {len(image_data_list)} ·∫£nh th·ªèa m√£n b·ªô l·ªçc.")
                progress_bar.empty()

# --- Hi·ªÉn th·ªã k·∫øt qu·∫£ v√† Ch·ª©c nƒÉng Ph√¢n t√≠ch AI ---

if st.session_state.extracted_images:
    
    # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng ·∫£nh
    st.subheader(f"K·∫øt qu·∫£ L·ªçc: {len(st.session_state.extracted_images)} H√¨nh ·∫£nh")

    # N√∫t Ph√¢n t√≠ch AI
    if st.button("ü§ñ Ph√¢n t√≠ch H√¨nh ·∫£nh b·∫±ng AI", disabled=not st.session_state.extracted_images):
        st.session_state.analyzed_images = []
        with st.spinner("ƒêang g·ªçi API Gemini ƒë·ªÉ ph√¢n t√≠ch h√¨nh ·∫£nh..."):
            
            analysis_progress = st.progress(0)
            
            for i, img_info in enumerate(st.session_state.extracted_images):
                
                description = analyze_image_with_ai(img_info['base64'], img_info['mime_type'])
                
                analyzed_item = img_info.copy()
                analyzed_item['analysis'] = description
                st.session_state.analyzed_images.append(analyzed_item)
                
                analysis_progress.progress((i + 1) / len(st.session_state.extracted_images))
            
            st.success("Ho√†n t·∫•t ph√¢n t√≠ch AI cho t·∫•t c·∫£ h√¨nh ·∫£nh ƒë√£ l·ªçc.")
            analysis_progress.empty()

    
    # --- Hi·ªÉn th·ªã ·∫£nh v√† Metadata ---
    
    display_list = st.session_state.analyzed_images if st.session_state.analyzed_images else st.session_state.extracted_images
    
    st.markdown("---")
    st.subheader("Xem tr∆∞·ªõc v√† T·∫£i h√†ng lo·∫°t")
    
    # T·∫£i ·∫£nh h√†ng lo·∫°t (t·∫°o t·ªáp ZIP ·∫£o)
    if display_list:
        from io import BytesIO
        import zipfile
        
        @st.cache_data
        def create_zip_archive(data_list):
            """T·∫°o t·ªáp ZIP trong b·ªô nh·ªõ."""
            zip_buffer = BytesIO()
            with zipfile.ZipFile(zip_buffer, "a", zipfile.ZIP_DEFLATED, False) as zip_file:
                for i, item in enumerate(data_list):
                    # ƒê·∫∑t t√™n file b·∫±ng index + k√≠ch th∆∞·ªõc + m√¥ t·∫£ (n·∫øu c√≥)
                    file_name_prefix = f"{i+1}_{item['width']}x{item['height']}"
                    
                    # N·∫øu c√≥ m√¥ t·∫£ AI, th√™m n√≥ v√†o m·ªôt t·ªáp TXT
                    if 'analysis' in item and item['analysis'] != 'Ch∆∞a ph√¢n t√≠ch':
                         txt_content = f"URL g·ªëc: {item['url']}\nK√≠ch th∆∞·ªõc: {item['width']}x{item['height']} pixels\nM√¥ t·∫£ AI:\n{item['analysis']}"
                         zip_file.writestr(f"{file_name_prefix}_description.txt", txt_content.encode('utf-8'))

                    # Th√™m h√¨nh ·∫£nh
                    ext = ".jpg" if "jpeg" in item['mime_type'] else ".png" if "png" in item['mime_type'] else ".bin"
                    zip_file.writestr(f"{file_name_prefix}{ext}", item['bytes'])
            
            return zip_buffer.getvalue()

        zip_bytes = create_zip_archive(display_list)
        
        st.download_button(
            label=f"‚¨áÔ∏è T·∫£i {len(display_list)} ·∫¢nh v√† M√¥ t·∫£ (ZIP)",
            data=zip_bytes,
            file_name="trich_xuat_hinh_anh.zip",
            mime="application/zip",
            use_container_width=False
        )
        st.markdown(f"*(T·ªáp ZIP ch·ª©a {len(display_list)} ·∫£nh v√† {len(st.session_state.analyzed_images)} t·ªáp m√¥ t·∫£ AI n·∫øu ƒë√£ ph√¢n t√≠ch)*")


    # Hi·ªÉn th·ªã t·ª´ng ·∫£nh trong m·ªôt b·ªë c·ª•c l∆∞·ªõi
    cols = st.columns(3)
    
    for i, item in enumerate(display_list):
        col = cols[i % 3]
        
        with col:
            # T·∫°o ƒë∆∞·ªùng d·∫´n data URL ƒë·ªÉ hi·ªÉn th·ªã
            img_data_url = f"data:{item['mime_type']};base64,{item['base64']}"
            st.image(img_data_url, caption=f"K√≠ch th∆∞·ªõc: {item['width']}x{item['height']}", use_column_width=True)
            
            if item.get('analysis', 'Ch∆∞a ph√¢n t√≠ch') != 'Ch∆∞a ph√¢n t√≠ch':
                with st.expander("Ph√¢n t√≠ch AI"):
                    st.markdown(item['analysis'])
            
            # T·∫£i t·ª´ng ·∫£nh ri√™ng l·∫ª
            st.download_button(
                label="T·∫£i xu·ªëng",
                data=item['bytes'],
                file_name=f"image_{i+1}_{item['width']}x{item['height']}.jpg",
                mime=item['mime_type'],
                key=f"download_{i}",
                use_container_width=True
            )
            st.markdown("---")

else:
    if 'extracted_images' in st.session_state and st.session_state.extracted_images == [] and input_url:
         st.warning("Kh√¥ng t√¨m th·∫•y h√¨nh ·∫£nh n√†o th·ªèa m√£n b·ªô l·ªçc c·ªßa b·∫°n tr√™n trang web n√†y.")
